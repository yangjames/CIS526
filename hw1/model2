#!/usr/bin/python
import optparse
import sys
from collections import defaultdict

# parse command line inputs 
optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()

# get file names of documents
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

# split documents by line and split sentences into words
sys.stderr.write("Training with Dice's coefficient...")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]

# create hash maps for each unique french word count per sentence,
# each unique english word count per sentence,
# and occurrence of each unique french english pair per sentence
f_count = defaultdict(int)
e_count = defaultdict(int)
fe_count = defaultdict(int)
for (n, (f, e)) in enumerate(bitext):
  for f_i in set(f):
    f_count[f_i] += 1
    for e_j in set(e):
      fe_count[(f_i,e_j)] += 1
  for e_j in set(e):
    e_count[e_j] += 1
  if n % 500 == 0:
    sys.stderr.write(".")

# intialize t(f|e) uniformly - pseudocode line 1
t_e_given_f = defaultdict(float)
for (f,e) in bitext:
  for e_i in set(e):
    for f_j in set(f):
      t_e_given_f[(e_i,f_j)] = float(1)/len(f_count)

# begin EM
iterator = 0
max_iter = 6
while iterator < max_iter: # pseudocode line 2
  # intialize - pseudocode line 3
  count_e_given_f = defaultdict(float)
  total_f = defaultdict(float)
  for (f,e) in bitext:
    for e_i in set(e):
      for f_j in set(f):
        count_e_given_f[(e_i,f_j)] = 0
    for f_j in set(f):
      total_f[f_j] = 0

  # pseudocode line 6
  for (f,e) in bitext:
    s_total = defaultdict(float)
    # compute normalization
    for e_i in set(e):
      s_total[e_i] = 0
      for f_j in set(f):
        s_total[e_i] += t_e_given_f[(e_i,f_j)]
    # collect counts
    for e_i in set(e):
      for f_j in set(f):
        count_e_given_f[(e_i,f_j)] += t_e_given_f[(e_i,f_j)]/s_total[e_i]
        total_f[f_j] += t_e_given_f[(e_i,f_j)]/s_total[e_i]

  # estimate probabilities
  for (e_i,f_j) in set(t_e_given_f.keys()):
    t_e_given_f[(e_i,f_j)] = count_e_given_f[(e_i,f_j)]/total_f[f_j]
  iterator += 1

# intialize a(i|j,le,lf) uniformly
a_i_given_jlelf = defaultdict(float)
for (f,e) in bitext:
  for (i,e_i) in enumerate(e):
    for (j,f_j) in enumerate(f):
      a_i_given_jlelf[(i,j,len(e),len(f))] = 1/float(len(f)+1)

# begin model 2 training
iterator = 0
max_iter = 6
while iterator < max_iter:
  count_e_given_f = defaultdict(float)
  total_f = defaultdict(float)
  count_a_i_given_jlelf = defaultdict(float)
  total_a_jlelf = defaultdict(float)
  for (f,e) in bitext:
    for (i,e_i) in enumerate(e):
      for (j,f_j) in enumerate(f):
        count_e_given_f[(e_i,f_j)] = 0
        total_f[f_j] = 0
        count_a_i_given_jlelf[(i,j,len(e),len(f))] = 0
        total_a_jlelf[(j,len(e),len(f))] = 0
  for (f,e) in bitext:
    s_total = defaultdict(float)
    for (j,e_j) in enumerate(e):
      s_total[(j,e_j)] = 0.0
      for (i,f_i) in enumerate(f):
        s_total[(j,e_j)] += t_e_given_f[(e_j,f_i)]*a_i_given_jlelf[(i,j,len(e),len(f))]
    for (j,e_j) in enumerate(e):
      for (i,f_i) in enumerate(f):
        if s_total[(j,e_j)] > 0:
          c = t_e_given_f[(e_j,f_i)] * a_i_given_jlelf[(i,j,len(e),len(f))] / s_total[(j,e_j)]
        else:
          c = 0
        count_e_given_f[(e_j,f_i)] += c
        total_f[f_i] += c
        count_a_i_given_jlelf[(i,j,len(e),len(f))] += c
        total_a_jlelf[(j,len(e),len(f))] += c
  t_e_given_f = defaultdict(float)
  a_i_given_jlelf = defaultdict(float)
  for (f,e) in bitext:
    for e_i in set(e):
      for f_j in set(f):
        if total_f[f_j] > 0:
          t_e_given_f[(e_i,f_j)] = count_e_given_f[(e_i,f_j)]/total_f[f_j]
        else:
          t_e_given_f[(e_i,f_j)] = 0
  for (f,e) in bitext:
    for (i,e_i) in enumerate(e):
      for (j,f_j) in enumerate(f):
        if total_a_jlelf[(j,len(e),len(f))] > 0:
          a_i_given_jlelf[(i,j,len(e),len(f))] = count_a_i_given_jlelf[(i,j,len(e),len(f))]/total_a_jlelf[(j,len(e),len(f))]
        else:
          a_i_given_jlelf[(i,j,len(e),len(f))] = 0
  iterator += 1

# print out alignment
for (f, e) in bitext:
  for (i, e_i) in enumerate(e): 
    biggest = 0.0
    idx = 0
    for (j, f_j) in enumerate(f):
      if t_e_given_f[(e_i,f_j)]*a_i_given_jlelf[(i,j,len(e),len(f))] > biggest:
        biggest =  t_e_given_f[(e_i,f_j)]*a_i_given_jlelf[(i,j,len(e),len(f))]
        idx = j
    sys.stdout.write("%i-%i " % (idx,i))
  sys.stdout.write("\n")
