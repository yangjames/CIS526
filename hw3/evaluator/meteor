#!/usr/bin/env python
import argparse # optparse is deprecated
from itertools import islice # slicing for iterators
import sys
from nltk import pos_tag, word_tokenize, download
from nltk.corpus import wordnet as wn
from collections import defaultdict

def word_matches(h, ref):
    """
    r_str = ' '.join(ref)
    h_str = ' '.join(h)
    r_pos = pos_tag(word_tokenize(r_str))
    h_pos = pos_tag(word_tokenize(h_str))
    sys.stderr.write(str(r_pos) + '\n')
    sys.stderr.write(str(h_pos) + '\n\n')
    """
    """
    h = [word.lower() for word in h]
    ref = [word.lower() for word in ref]
    """
    # parameters
    alpha = 0.82
    beta = 0.2
    gamma = 0.21

    # generate all possible chunks from reference sentence
    r_dict = [[] for x in xrange(len(ref)*(len(ref)+1)/2)]
    idx = 0
    for r_start in xrange(0,len(ref)+1):
        for r_end in xrange(r_start+1, len(ref)+1):
            r_dict[idx] = ref[r_start:r_end]
            idx += 1

    # look for indices with words not present and add synonym to dictionary
    indices = [False for i,word in enumerate(h)]
    for i,word in enumerate(h):
        if [word] not in r_dict:
            indices[i] = True
    
    syn_count = 0
    for i,indicator in enumerate(indices):
        if indicator:
            word = h[i]
            try:
                for ss in wn.synsets(word):
                    synonym = ss.lemma_names()[0].decode('utf-8')
                    r_dict.append([synonym])
                    if synonym in ref:
                        syn_count += 1
                        #h[i]=synonym
                break
            except ValueError:
                sys.stderr.write('')
    
    # generate bag of words for reference and hypothesis sentences
    r_bag = defaultdict(float)
    for r_word in ref:
        if r_word in r_bag:
            r_bag[r_word] += 1.0
        else:
            r_bag[r_word] = 1.0
    
    h_bag = defaultdict(float)
    for h_word in h:
        if h_word in h_bag:
            h_bag[h_word] += 1.0
        else:
            h_bag[h_word] = 1.0
            
    # get number of chunks
    c = 0.0
    h_start = 0
    h_end = 1
    while h_end < len(h)+1:
        potential_chunk = h[h_start:h_end]
        if potential_chunk in r_dict:
            h_end += 1
        else:
            c += 1
            h_start = h_end+1
            h_end = h_start+1

    # calculate word count
    m = sum(1.0 for w in h if w in ref)

    # calculate intersection of reference and hypothesis sentences
    hr_intersect = defaultdict(float)
    for r_word in r_bag:
        if r_word in h_bag:
            hr_intersect[r_word] = min(h_bag[r_word],r_bag[r_word])
    
    i = 0.0
    for word in hr_intersect:
        i += hr_intersect[word]

    # calculate translation precision
    P_he = i/len(set(h))

    # calculate translation recall
    R_he = i/len(set(ref))

    # catch divide-by-zero error
    if i == 0.0:
        return 0.0
    #return (1.0-gamma*((c/m) ** beta))*P_he*R_he/((1.0-alpha)*R_he+alpha*P_he)
    #alpha = 0.5
    return P_he*R_he/((1.0-alpha)*R_he+alpha*P_he)
 
def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
            help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
 
    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [sentence.strip().split() for sentence in pair.split(' ||| ')]
 
    # note: the -n option does not work in the original code
    for i,(h1, h2, ref) in enumerate(islice(sentences(), opts.num_sentences)):
        sys.stderr.write('\rsentence %d'  % i)
        h1_match = word_matches(h1, ref)
        h2_match = word_matches(h2, ref)
        print(1 if h1_match > h2_match else # \begin{cases}
                (-1 if h1_match == h2_match
                    else -1)) # \end{cases}
 
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
